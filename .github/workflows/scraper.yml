name: Alumni Scraper - Periodic Update

on:
  # Run every 6 months (January and July on the 1st)
  schedule:
    - cron: '0 0 1 1,7 *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      batch:
        description: 'Specific batch to update (optional)'
        required: false
        type: string
      max_profiles:
        description: 'Maximum profiles to scrape'
        required: false
        default: '100'
        type: string
      force_update:
        description: 'Force update all profiles'
        required: false
        default: 'false'
        type: boolean

jobs:
  scrape-linkedin:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hour timeout for long scraping jobs
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: ${{ secrets.DB_PASSWORD }}
          POSTGRES_DB: alumni_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps

      - name: Initialize database
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: alumni_db
          DB_USER: postgres
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          python -c "from alumni_system.database.init_db import init_database; init_database()"

      - name: Run LinkedIn scraper
        env:
          # Database credentials
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: alumni_db
          DB_USER: postgres
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          # LinkedIn credentials
          LINKEDIN_EMAIL: ${{ secrets.LINKEDIN_EMAIL }}
          LINKEDIN_PASSWORD: ${{ secrets.LINKEDIN_PASSWORD }}
          # B2 Storage credentials
          B2_APPLICATION_KEY_ID: ${{ secrets.B2_APPLICATION_KEY_ID }}
          B2_APPLICATION_KEY: ${{ secrets.B2_APPLICATION_KEY }}
          B2_BUCKET_NAME: ${{ secrets.B2_BUCKET_NAME }}
          # Scraper settings
          SCRAPER_HEADLESS: 'true'
          SCRAPER_MIN_DELAY: '10'
          SCRAPER_MAX_DELAY: '30'
        run: |
          python -m alumni_system.scraper.run \
            --batch "${{ github.event.inputs.batch || '' }}" \
            --max-profiles "${{ github.event.inputs.max_profiles || '100' }}" \
            ${{ github.event.inputs.force_update == 'true' && '--force-update' || '' }}

      - name: Generate scraping report
        if: always()
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: alumni_db
          DB_USER: postgres
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          python -c "
          from alumni_system.database.connection import get_db_context
          from alumni_system.database.crud import get_scraping_logs
          from datetime import datetime, timedelta
          
          with get_db_context() as db:
              recent_logs = get_scraping_logs(db, limit=100)
              
              # Count statistics
              today = datetime.utcnow().date()
              today_logs = [l for l in recent_logs if l.created_at.date() == today]
              
              success = len([l for l in today_logs if l.status == 'success'])
              failed = len([l for l in today_logs if l.status == 'failed'])
              
              print(f'Scraping Report for {today}')
              print(f'========================')
              print(f'Total processed: {len(today_logs)}')
              print(f'Successful: {success}')
              print(f'Failed: {failed}')
              print(f'Success rate: {success/(success+failed)*100 if (success+failed) > 0 else 0:.1f}%')
          "

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraping-logs-${{ github.run_id }}
          path: |
            *.log
          retention-days: 30

      - name: Send notification on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'LinkedIn Scraper Failed - ' + new Date().toISOString().split('T')[0],
              body: `The scheduled LinkedIn scraping job has failed.\n\nWorkflow Run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}\n\nPlease check the logs for details.`,
              labels: ['bug', 'automation']
            })
